ğŸ“„ Customer Data Cleaning Pipeline

A fully automated text-cleaning system designed to preprocess raw customer feedback at scale.
This project removes noise such as emojis, HTML tags, repeated characters, special symbols, contractions, and unnecessary stopwords â€” while correctly preserving negation meaning (e.g., â€œdidnâ€™tâ€ â†’ â€œdid notâ€).

The project includes:

âœ”ï¸ A complete Python cleaning pipeline
âœ”ï¸ A dataset generator for large-scale testing
âœ”ï¸ A Streamlit web application for uploading & cleaning CSV files
âœ”ï¸ Before/after comparison for all comments
âœ”ï¸ Downloadable cleaned dataset

ğŸ“Œ Features

Clean customer comments in bulk (1000+ comments supported)

Remove emojis, HTML tags (but keep text), noise & repeated characters

Expand contractions correctly (didnâ€™t â†’ did not, weâ€™re â†’ we are)

Intelligent stopword removal while preserving negation words

Before/After comparison table for every comment

Summary metrics (word count reduction, total words removed, etc.)

Download the cleaned dataset as CSV

Dataset generator for creating synthetic comments

ğŸ§  Why This Pipeline?

Raw customer comments typically contain noise that disrupts NLP workflows:

Emojis, tags, special characters

Broken grammar, contractions, and repeated letters

Inconsistent casing

Words inside HTML tags

Unnecessary filler text

Manual cleaning is time-consuming and inconsistent.
This pipeline ensures clean, reliable, NLP-ready text with zero manual effort.

ğŸš€ Project Structure
Customer_cleaning_pipeline/
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py           # User-facing web interface
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cleaning_functions.py      # Core cleaning pipeline logic
â”‚   â”œâ”€â”€ main_pipeline.py           # CLI cleaning script
â”‚   â”œâ”€â”€ data_generator.py          # Creates sample dataset
â”‚   â””â”€â”€ generate_large_dataset.py  # Generates 1500+ synthetic comments
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_comments.csv        # Input dataset example
â”‚   â””â”€â”€ cleaned_output.csv         # Output (created after cleaning)
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .gitignore                     # Git ignore rules
â””â”€â”€ README.md                      # Project documentation

ğŸ§© Tech Stack

Python 3.10+

NLTK (tokenization, stopwords)

contractions (expanding contractions)

emoji (emoji removal)

Streamlit (web UI)

Pandas

âš™ï¸ How to Run the Project
1ï¸âƒ£ Clone the repository
git clone https://github.com/RehanaZerdi/Customer_cleaning_pipeline.git
cd Customer_cleaning_pipeline

2ï¸âƒ£ Create a virtual environment
python -m venv venv
venv\Scripts\activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Run the Streamlit app
streamlit run app/streamlit_app.py

5ï¸âƒ£ Run the cleaning pipeline (CLI)
python src/main_pipeline.py

ğŸ§¼ Cleaning Steps (Behind the Scenes)

The comment cleaning pipeline performs:

Expand contractions

Remove HTML tags (keep text inside)

Remove emojis

Normalize repeated characters

Convert to lowercase

Strip special characters

Stopword removal with negation protection

Final whitespace cleanup

ğŸ“Š Output Examples

Before:

Didn't meet expectations weren't ğŸ˜¡ğŸ˜¡ <div>Gooood quality though</div>


After:

did not meet expectations were not goood quality

ğŸ“¥ Web App Screenshot

You can add your screenshot here later:

![Streamlit UI](path_to_screenshot)

ğŸ“ Download Options

Download fully cleaned CSV

View before/after results

Review statistics summary

ğŸ“œ License

This project is published under the MIT License.

ğŸ™Œ Acknowledgements

Special thanks to the guidance provided during the internship project at Newton AI Technologies.